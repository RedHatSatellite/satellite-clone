---
- include_tasks: pre_install_check.yml
  when: run_pre_install_check

- include_tasks: interface_check.yml
  when: run_pre_install_check and check_networking_interfaces

- include_tasks: backup_satellite_version_check.yml

- include_tasks: backup_check.yml

- name: Set Satellite version dependent variables
  include_vars: "satellite_{{ satellite_version }}.yml"

- name: Identify the hostname from the backup config tar file
  get_value_from_yaml_in_tarball:
    tarball: "{{ config_files_path }}"
    target_file: "etc/foreman-proxy/settings.yml"
    keys: [":foreman_url"]
  register: backup_hostname

- name: Set backup_hostname variable
  set_fact:
    hostname: "{{ backup_hostname.value | urlsplit('hostname') }}"

- name: Check that the hostname is not none
  fail: msg="Unable to derive Satellite hostname from the backup config file - value ({{ hostname }}) doesn't look right"
  when: backup_hostname|length == 0

- name: Check that the registration variables (activationkey, org) are updated
  fail: msg="Please update the variables in /etc/satellite-clone/satellite-clone-vars.yml"
  when: ((activationkey == "changeme") or (org == "changeme")) and register_to_portal

- name: Register/Subscribe the system to Red Hat Portal
  command: "subscription-manager register --force --activationkey='{{ activationkey }}' --org '{{ org }}'"
  when: register_to_portal

- name: Disable all repos
  command: subscription-manager repos --disable "*"
  ignore_errors: True
  register: disable_repos_result
  when: enable_repos

- name: set fact - disable_repos_result
  set_fact:
    disable_repos_result_fail: "{{ (disable_repos_result.rc | int) != 0 }}"
    caceable: True
  when: enable_repos

- name: Check that the repos are disabled without errors
  fail:
    msg: "Disabling repos failed. Make sure that your system is registered to Red Hat and has the appropriate Satellite
         subscriptions attached.  Alternatively, you can skip this task by specifying `enable_repos` to `False` if
         you would like to enable the required repositories manually."
  when: enable_repos and disable_repos_result_fail

- name: Enable required repos for Satellite installation
  command: subscription-manager repos --enable rhel-{{ ansible_distribution_major_version }}-server-rpms --enable rhel-server-rhscl-{{ ansible_distribution_major_version }}-rpms --enable rhel-{{ ansible_distribution_major_version }}-server-satellite-{{ satellite_version }}-rpms --enable rhel-7-server-satellite-maintenance-6-rpms --enable {{ ansible_repository_label }}
  ignore_errors: True
  register: enable_repos_result
  when: enable_repos

- name: set fact - enable_repos_result
  set_fact:
    enable_repos_result_fail: "{{ (enable_repos_result.rc | int) != 0}}"
    caceable: True
  when: enable_repos

- name: Check that the repos are enabled without errors
  fail:
    msg: "Enabling required repos failed.  Make sure that your system has access to the following repos:
         rhel-{{ ansible_distribution_major_version }}-server-rpms,
         rhel-server-rhscl-{{ ansible_distribution_major_version }}-rpms,
         rhel-{{ ansible_distribution_major_version }}-server-satellite-{{ satellite_version }}-rpms.
         Alternatively, you can skip this task by specifying `enable_repos` to `False` and provide alternate
         means (e.g., custom repositories) to get the required rpms."
  when: enable_repos and enable_repos_result_fail

# Remove EPEL as it causes problems for the Satellite installer
- name: Remove epel
  yum_repository:
    name: epel
    state: absent

- name: Clean yum info
  command: yum clean all

# turn off firewall
- name: turn off firewalld - rhel7
  service: name=firewalld enabled=no state=stopped
  when: ansible_distribution_major_version == "7" and disable_firewall

# Update hostname
- name: set host_name
  hostname: name={{ hostname }}
- name: check /etc/hostname
  lineinfile: dest=/etc/hostname line={{ hostname }}
  when: ansible_distribution_major_version == "7"

- name: create /etc/hosts
  template: src=hosts.j2 dest=/etc/hosts
  when: overwrite_etc_hosts

# Install Satellite packages
- name: Install Satellite packages
  yum:
    name: "{{ satellite_package }}"
    state: latest

- name: Install packages necessary to check DB status
  yum:
    name: "{{ db_packages | join(',') }}"
    state: present
  when: db_packages | length > 0

# The postgres user is created after installing postgresql packages, so
# we perform this owner/group change at this point rather than earlier
- name: change owner of backup directory to postgres
  file:
    path: "{{ backup_dir }}"
    owner: postgres
    group: postgres
    recurse: yes

- name: test foreman.dump file is readable by postgres user
  command: "test -r {{ backup_dir }}/foreman.dump"
  become: yes
  become_user: postgres
  register: access_foreman_dump
  ignore_errors: yes
  when: clone_foreman_dump_exists

- name: setting fact - access_foreman_dump
  set_fact:
    clone_no_foreman_dump_access: "{{ access_foreman_dump is failed }}"

- name: test candlepin.dump file is readable by postgres user
  command: "test -r {{ backup_dir }}/candlepin.dump"
  become: yes
  become_user: postgres
  register: access_candlepin_dump
  ignore_errors: yes
  when: clone_candlepin_dump_exists

- name: setting fact - access_candlepin_dump
  set_fact:
    clone_no_candlepin_dump_access: "{{ access_candlepin_dump is failed }}"

- name: fail if postgres user doesn't have access to files
  fail:
    msg: >
      The postgres user does not have access to the files in {{ backup_dir }}.
      Please move the backup directory to a different directory with the correct
      permissions. Avoid using /root.
  when: clone_foreman_dump_exists and clone_candlepin_dump_exists and (clone_no_foreman_dump_access or clone_no_candlepin_dump_access)

  # Because of https://projects.theforeman.org/issues/30506
- name: Install Satellite SELinux packages for 6.8 and above
  yum:
    name: "{{ selinux_packages | join(',') }}"
    state: present
  when: selinux_packages | length > 0

- name: untar config files (for cloning only)
  command: tar --selinux --overwrite -xf {{ backup_dir }}/config_files.tar.gz -C / --exclude=var/lib/foreman/public

# postgres is not happy when initializing if the data folder isn't empty
- name: remove postgresql.conf
  file:
    path: /var/lib/pgsql/data/postgresql.conf
    state: absent
  when: satellite_version in ["6.6", "6.7"]

# postgres is not happy when initializing if the data folder isn't empty
- name: remove postgresql.conf
  file:
    path: /var/opt/rh/rh-postgresql12/lib/pgsql/data/postgresql.conf
    state: absent
  when: satellite_version in ["6.8", "6.9", "6.10"]

- name: Restore selinux context on the filesystem
  command: restorecon -R /
  when: restorecon

# This file tells the candlepin puppet module in the Satellite installer that
# candlepin is already set up. We remove it so candlepin is set up correctly.
- name: Remove cpdb_done file
  file:
    path: "{{ item }}"
    state: absent
  with_items:
    - /var/lib/candlepin/cpdb_done
    - /var/lib/candlepin/.puppet-candlepin-cpdb-create-done

# jmx.conf can cause issues when re-running the installer later in the playbook
- name: Remove jmx.conf file
  file:
    state: absent
    path: /etc/tomcat/conf.d/jmx.conf

# This no longer is supported in 6.6, but was still found enabled in some backups
- name: Remove hammer csv module
  file:
    state: absent
    path: /etc/hammer/cli.modules.d/csv.yml
  when: satellite_version in ["6.6"]

- name: Run Satellite installer
  environment:
    HOSTNAME: "{{ hostname }}"
  command: >
    {{ satellite_installer_cmd }}
    {{ (satellite_scenario is defined) | ternary("--scenario", "") }} {{ satellite_scenario }}
    --foreman-proxy-dns false
    --foreman-proxy-dhcp false
    --foreman-proxy-tftp false
    {{ satellite_installer_options | default("") }}

- block:
  # Cron services can interfere with restoring the backup
  - name: stop crond
    systemd:
      name: crond
      state: stopped

  - name: restore using foreman-maintain
    command: foreman-maintain restore --assumeyes {{ backup_dir }}

  - name: start crond
    systemd:
      name: crond
      state: started

  - name: Restart services
    command: foreman-maintain service start

    # We don't use pause module here because it displays a prompt
    # to use ctrl+c which would break out of a production script.
  - name: Wait for foreman-tasks service to start
    command: sleep 300

  - name: Cleanup paused tasks
    command: foreman-rake foreman_tasks:cleanup TASK_SEARCH='label ~ *' STATES='paused'  VERBOSE=true AFTER='0h'  VERBOSE=true

  - name: Check if foreman-maintain-hammer.yml exists
    stat:
      path: /etc/foreman-maintain/foreman-maintain-hammer.yml
    register: foreman_maintain_hammer

  - name: Backup existing foreman-maintain-hammer credentials
    command: mv /etc/foreman-maintain/foreman-maintain-hammer.yml /etc/foreman-maintain/foreman-maintain-hammer.yml.satclone.backup
    when: foreman_maintain_hammer.stat.exists

  - name: Reset admin password
    command: foreman-rake permissions:reset password=changeme

  - name: Setup hammer for foreman-maintain
    lineinfile:
      path: /etc/foreman-maintain/foreman-maintain-hammer.yml
      state: present
      create: yes
      line: "---\n:foreman:\n  :username: admin\n  :password: changeme"

  - name: Run installer upgrade to match latest z-version
    command: '{{ satellite_installer_cmd }} {{ satellite_upgrade_options | default("") }}'
    environment:
      HOSTNAME: "{{ hostname }}"

  - name: Test Satellite
    command: hammer ping

  - include_tasks: reset_pulp_data.yml
    when:
      - not clone_pulp_data_exists
      - not online_backup
      - satellite_version in ["6.6", "6.7", "6.8", "6.9"]

  - name: Wait 60 seconds for services to be fully up
    wait_for: timeout=60

  - name: Disassociate capsules with lifecycle environments (to avoid the cloned Satellite server talking with live capsules)
    script: disassociate_capsules.rb
    register: disassociate_capsules

  - name: set fact - disassociate_capsules
    set_fact:
      clone_disassociate_capsules: "{{ disassociate_capsules.stdout }}"

  - name: copy disassociate_capsules.rb output
    local_action: copy content={{ clone_disassociate_capsules }} dest={{ playbook_dir }}/logs/reassociate_capsules.txt

  - debug:
      msg: "****NOTE**** Your Satellite's hostname is updated to match the original Satellite
            ****NOTE**** Your Satellite's password is updated to changeme"
  rescue:
  # After the installer runs, if there is an error, we want to disassociate capsules from lifecycle environments.
  # This is to prevent the clone trying to sync with existing original capsules. We do this with SQL in case
  # hammer is not able to execute a command.
  - name: Disassociate capsules from lifecycle environments to avoid communication with production capsules
    command: psql foreman -c "delete from katello_capsule_lifecycle_environments where capsule_id in (select smart_proxy_id from smart_proxy_features where feature_id = (select id from features where features.name = 'Pulp Node'));"
    become_user: postgres
    become: true

  - fail:
      msg: "Something went wrong! Please check the output above for more information."

- name: "Run katello {{ verify_rake_task }} for Satellite - Note that this might take hours"
  command: "foreman-rake katello:{{ verify_rake_task }} --trace"
  when: run_katello_reindex or not clone_pulp_data_exists
